use clap::Parser;\nuse mullama::{Model, ContextParams};\nuse anyhow::Result;\n\n#[derive(Parser)]\n#[command(author, version, about, long_about = None)]\nstruct Args {\n    /// Path to the GGUF model file\n    #[arg(short, long)]\n    model: String,\n\n    /// Prompt text to generate from\n    #[arg(short, long, default_value = \"Hello, world!\")]\n    prompt: String,\n\n    /// Number of tokens to generate\n    #[arg(short, long, default_value = \"100\")]\n    tokens: usize,\n\n    /// Number of threads to use\n    #[arg(short = 't', long, default_value = \"4\")]\n    threads: i32,\n}\n\nfn main() -> Result<()> {\n    let args = Args::parse();\n    \n    println!(\"Loading model from: {}\", args.model);\n    let model = Model::load(&args.model)?;\n    \n    println!(\"Creating context with {} threads\", args.threads);\n    let mut context_params = ContextParams::default();\n    context_params.n_threads = args.threads;\n    let mut ctx = model.create_context(context_params)?;\n    \n    println!(\"Tokenizing prompt: {}\", args.prompt);\n    let tokens = model.tokenize(&args.prompt)?;\n    \n    println!(\"Generating {} tokens...\", args.tokens);\n    let result = ctx.generate(&tokens, args.tokens)?;\n    \n    println!(\"\\nGenerated text:\\n{}\", result);\n    \n    Ok(())\n}