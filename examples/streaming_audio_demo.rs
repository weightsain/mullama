//! # Streaming Audio Integration Demo
//!
//! This example demonstrates real-time audio streaming capabilities with format conversion,
//! voice activity detection, and live audio processing for LLM inference.
//!
//! Features demonstrated:
//! - Real-time audio capture with CPAL
//! - Voice activity detection and noise reduction
//! - Format conversion between audio formats
//! - Integration with multimodal LLM processing
//! - WebSocket streaming for remote audio processing
//! - Performance monitoring and metrics
//!
//! Run with: cargo run --example streaming_audio_demo --features "full,streaming-audio"

use mullama::prelude::*;
use std::sync::Arc;
use std::time::Duration;
use tokio::time::{sleep, timeout};

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
use mullama::{
    AudioChunk, AudioConverter, AudioConverterConfig, AudioFormat, AudioInput, AudioStream,
    AudioStreamConfig, ConversionConfig, DevicePreference, MultimodalProcessor,
    StreamingAudioProcessor, StreamingMetrics,
};

#[tokio::main]
async fn main() -> Result<(), MullamaError> {
    println!("ğŸµ Streaming Audio Integration Demo");
    println!("===================================");

    #[cfg(all(
        feature = "streaming-audio",
        feature = "multimodal",
        feature = "format-conversion"
    ))]
    {
        // 1. Setup streaming audio configuration
        let audio_config = setup_audio_configuration();

        // 2. Initialize streaming audio processor
        let mut audio_processor = initialize_audio_processor(audio_config).await?;

        // 3. Setup format conversion
        let audio_converter = setup_audio_converter().await?;

        // 4. Setup multimodal processor for AI inference
        let multimodal_processor = setup_multimodal_processor().await?;

        // 5. Demonstrate real-time audio streaming
        demonstrate_real_time_streaming(&mut audio_processor, &multimodal_processor).await?;

        // 6. Demonstrate format conversion with streaming
        demonstrate_format_conversion_streaming(&audio_converter, &mut audio_processor).await?;

        // 7. Demonstrate voice activity detection
        demonstrate_voice_activity_detection(&mut audio_processor).await?;

        // 8. Show performance metrics
        show_performance_metrics(&audio_processor).await?;

        // 9. Demonstrate WebSocket integration
        demonstrate_websocket_integration().await?;

        // 10. Advanced streaming patterns
        demonstrate_advanced_streaming_patterns().await?;

        // Cleanup
        audio_processor.stop_capture().await?;
    }

    #[cfg(not(all(
        feature = "streaming-audio",
        feature = "multimodal",
        feature = "format-conversion"
    )))]
    {
        println!(
            "âŒ This demo requires streaming-audio, multimodal, and format-conversion features"
        );
        println!("Run with: cargo run --example streaming_audio_demo --features \"full,streaming-audio\"");
    }

    println!("\nâœ¨ Streaming audio demo completed!");
    Ok(())
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
fn setup_audio_configuration() -> AudioStreamConfig {
    println!("\nğŸ”§ Setting up Audio Configuration");
    println!("=================================");

    let config = AudioStreamConfig::new()
        .sample_rate(16000) // Optimal for speech processing
        .channels(1) // Mono for efficiency
        .buffer_size(512) // Low latency
        .enable_noise_reduction(true)
        .enable_voice_detection(true)
        .vad_threshold(0.3) // Sensitive voice detection
        .max_latency_ms(50); // Real-time performance

    println!("âœ… Audio configuration:");
    println!("   ğŸ“Š Sample Rate: {} Hz", config.sample_rate);
    println!("   ğŸ”Š Channels: {}", config.channels);
    println!("   ğŸ“¦ Buffer Size: {} samples", config.buffer_size);
    println!("   ğŸ”‡ Noise Reduction: {}", config.enable_noise_reduction);
    println!("   ğŸ™ï¸ Voice Detection: {}", config.enable_voice_detection);
    println!("   âš¡ Max Latency: {} ms", config.max_latency_ms);

    config
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn initialize_audio_processor(
    config: AudioStreamConfig,
) -> Result<StreamingAudioProcessor, MullamaError> {
    println!("\nğŸ›ï¸ Initializing Audio Processor");
    println!("===============================");

    let mut processor = StreamingAudioProcessor::new(config)?;
    processor.initialize().await?;

    println!("âœ… Audio processor initialized");

    // List available devices
    match processor.list_input_devices() {
        Ok(devices) => {
            println!("ğŸ¤ Available input devices:");
            for (i, device) in devices.iter().enumerate() {
                println!("   {}. {}", i + 1, device);
            }
        }
        Err(_) => {
            println!("âš ï¸ Could not enumerate audio devices (feature may not be available)");
        }
    }

    Ok(processor)
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn setup_audio_converter() -> Result<AudioConverter, MullamaError> {
    println!("\nğŸ”„ Setting up Audio Converter");
    println!("=============================");

    let config = AudioConverterConfig::new()
        .max_concurrent_conversions(4)
        .enable_cache(true)
        .cache_size_mb(100);

    let converter = AudioConverter::new(config)?;

    println!("âœ… Audio converter configured");
    println!("   ğŸ”„ Max concurrent conversions: 4");
    println!("   ğŸ’¾ Cache enabled: 100 MB");

    Ok(converter)
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn setup_multimodal_processor() -> Result<MultimodalProcessor, MullamaError> {
    println!("\nğŸ­ Setting up Multimodal Processor");
    println!("==================================");

    let processor = MultimodalProcessor::new()
        .enable_image_processing()
        .enable_audio_processing()
        .build();

    println!("âœ… Multimodal processor ready for inference");
    println!("   ğŸ–¼ï¸ Image processing: enabled");
    println!("   ğŸµ Audio processing: enabled");

    Ok(processor)
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn demonstrate_real_time_streaming(
    audio_processor: &mut StreamingAudioProcessor,
    multimodal_processor: &MultimodalProcessor,
) -> Result<(), MullamaError> {
    println!("\nğŸŒŠ Real-time Audio Streaming Demo");
    println!("=================================");

    println!("ğŸ¤ Starting audio capture (simulated for 3 seconds)...");

    // Start audio stream
    let mut audio_stream = match audio_processor.start_capture().await {
        Ok(stream) => stream,
        Err(_) => {
            println!("âš ï¸ Audio capture not available (no audio devices or permissions)");
            println!("   Simulating audio stream instead...");
            return simulate_audio_stream(multimodal_processor).await;
        }
    };

    // Process audio chunks for a limited time
    let stream_duration = Duration::from_secs(3);
    let mut chunks_processed = 0;

    match timeout(stream_duration, async {
        while let Some(chunk) = audio_stream.next().await {
            chunks_processed += 1;

            // Process the audio chunk
            let processed_chunk = audio_processor.process_chunk(&chunk).await?;

            println!(
                "   ğŸ“¦ Chunk {}: {} samples, voice: {}, level: {:.3}",
                chunks_processed,
                processed_chunk.samples.len(),
                processed_chunk.voice_detected,
                processed_chunk.signal_level
            );

            // Convert to multimodal input for AI processing
            if processed_chunk.voice_detected {
                let audio_input = processed_chunk.to_audio_input();
                println!("   ğŸ¤– Processing with AI model (simulated)...");
                // In real scenario: let result = multimodal_processor.process_audio(&audio_input).await?;
                println!("   âœ… AI processing completed");
            }

            // Process a few chunks for demo
            if chunks_processed >= 5 {
                break;
            }
        }
        Ok::<(), MullamaError>(())
    })
    .await
    {
        Ok(_) => println!("âœ… Real-time streaming demo completed"),
        Err(_) => println!("â° Streaming demo timed out (this is normal for the demo)"),
    }

    println!("ğŸ“Š Processed {} audio chunks", chunks_processed);
    Ok(())
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn simulate_audio_stream(
    multimodal_processor: &MultimodalProcessor,
) -> Result<(), MullamaError> {
    println!("ğŸ­ Simulating audio stream processing...");

    for i in 1..=5 {
        // Create simulated audio chunk
        let samples: Vec<f32> = (0..512)
            .map(|x| (x as f32 * 0.01 * std::f32::consts::PI).sin() * 0.1)
            .collect();

        let chunk = AudioChunk::new(samples, 1, 16000);

        println!(
            "   ğŸ“¦ Simulated Chunk {}: {} samples, level: {:.3}",
            i,
            chunk.samples.len(),
            chunk.signal_level
        );

        // Simulate processing delay
        sleep(Duration::from_millis(100)).await;
    }

    println!("âœ… Simulated audio stream completed");
    Ok(())
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn demonstrate_format_conversion_streaming(
    audio_converter: &AudioConverter,
    audio_processor: &mut StreamingAudioProcessor,
) -> Result<(), MullamaError> {
    println!("\nğŸ”„ Format Conversion with Streaming");
    println!("===================================");

    // Simulate audio format conversion during streaming
    println!("ğŸµ Simulating real-time format conversion...");

    let sample_rates = vec![8000, 16000, 22050, 44100];
    let formats = vec!["WAV", "MP3", "FLAC"];

    for (i, (&sample_rate, format)) in sample_rates.iter().zip(formats.iter()).enumerate() {
        println!("   ğŸ”„ Converting to {} at {} Hz...", format, sample_rate);

        // Create sample audio data
        let samples: Vec<f32> = (0..1024)
            .map(|x| (x as f32 * 0.02 * std::f32::consts::PI).sin() * 0.2)
            .collect();

        let audio_input = AudioInput {
            samples,
            sample_rate: 16000,
            channels: 1,
            duration: 1024.0 / 16000.0,
            format: AudioFormat::WAV,
            transcript: None,
            metadata: std::collections::HashMap::new(),
        };

        // Simulate format conversion
        println!("   âœ… Conversion {} completed", i + 1);

        // Brief processing delay
        sleep(Duration::from_millis(50)).await;
    }

    println!("âœ… All format conversions completed");
    Ok(())
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn demonstrate_voice_activity_detection(
    audio_processor: &mut StreamingAudioProcessor,
) -> Result<(), MullamaError> {
    println!("\nğŸ™ï¸ Voice Activity Detection Demo");
    println!("================================");

    println!("ğŸ” Testing voice activity detection...");

    // Create test audio with different characteristics
    let test_scenarios = vec![
        ("Silence", vec![0.0; 512]),
        (
            "Noise",
            (0..512)
                .map(|_| rand::random::<f32>() * 0.05 - 0.025)
                .collect(),
        ),
        (
            "Speech-like",
            (0..512)
                .map(|x| {
                    if x % 50 < 25 {
                        (x as f32 * 0.1).sin() * 0.3
                    } else {
                        0.0
                    }
                })
                .collect(),
        ),
        (
            "Music",
            (0..512)
                .map(|x| (x as f32 * 0.05).sin() * 0.2 + (x as f32 * 0.08).cos() * 0.1)
                .collect(),
        ),
    ];

    for (scenario, samples) in test_scenarios {
        let mut chunk = AudioChunk::new(samples, 1, 16000);
        let processed_chunk = audio_processor.process_chunk(&chunk).await?;

        println!(
            "   ğŸ“Š {}: voice={}, level={:.3}",
            scenario, processed_chunk.voice_detected, processed_chunk.signal_level
        );
    }

    println!("âœ… Voice activity detection demo completed");
    Ok(())
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn show_performance_metrics(
    audio_processor: &StreamingAudioProcessor,
) -> Result<(), MullamaError> {
    println!("\nğŸ“Š Performance Metrics");
    println!("======================");

    let metrics = audio_processor.metrics().await;

    println!("ğŸ¯ Streaming Performance:");
    println!("   ğŸ“¦ Chunks processed: {}", metrics.chunks_processed);
    println!("   ğŸ”¢ Total samples: {}", metrics.total_samples);
    println!("   âŒ Dropped chunks: {}", metrics.dropped_chunks);
    println!(
        "   â±ï¸ Average latency: {:.1} ms",
        metrics.average_latency_ms
    );
    println!("   ğŸ“ˆ Max latency: {:.1} ms", metrics.max_latency_ms);
    println!(
        "   ğŸ™ï¸ Voice activity time: {:.1} s",
        metrics.voice_activity_time.as_secs_f32()
    );
    println!("   ğŸ”Š Noise events: {}", metrics.noise_events);

    // Calculate efficiency metrics
    let efficiency = if metrics.chunks_processed > 0 {
        ((metrics.chunks_processed - metrics.dropped_chunks) as f32
            / metrics.chunks_processed as f32)
            * 100.0
    } else {
        0.0
    };

    println!("   ğŸ“Š Stream efficiency: {:.1}%", efficiency);

    Ok(())
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn demonstrate_websocket_integration() -> Result<(), MullamaError> {
    println!("\nğŸŒ WebSocket Integration Demo");
    println!("============================");

    #[cfg(feature = "websockets")]
    {
        use mullama::{WebSocketConfig, WebSocketServer};

        println!("ğŸš€ Setting up WebSocket server for audio streaming...");

        let ws_config = WebSocketConfig::new()
            .port(8080)
            .max_connections(50)
            .enable_audio()
            .enable_compression();

        let server = WebSocketServer::new(ws_config).build().await?;

        println!("âœ… WebSocket server configured");
        println!("   ğŸ“ Audio streaming endpoint: ws://localhost:8080/ws/audio");
        println!("   ğŸ”„ Real-time audio processing ready");
        println!("   ğŸ“Š Max connections: 50");

        // Simulate WebSocket message handling
        println!("ğŸ“¡ Simulating WebSocket audio messages...");

        let message_types = vec![
            "Audio chunk received",
            "Voice activity detected",
            "Processing complete",
            "Response generated",
        ];

        for (i, msg_type) in message_types.iter().enumerate() {
            println!("   ğŸ“¨ Message {}: {}", i + 1, msg_type);
            sleep(Duration::from_millis(100)).await;
        }

        println!("âœ… WebSocket integration demo completed");
    }

    #[cfg(not(feature = "websockets"))]
    {
        println!("âš ï¸ WebSocket feature not available - skipping WebSocket demo");
    }

    Ok(())
}

#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn demonstrate_advanced_streaming_patterns() -> Result<(), MullamaError> {
    println!("\nğŸ¯ Advanced Streaming Patterns");
    println!("==============================");

    // Pattern 1: Buffered streaming with lookahead
    println!("1ï¸âƒ£ Buffered streaming with context...");
    println!("   ğŸ“¦ Maintaining 500ms audio buffer for context");
    println!("   ğŸ” Looking ahead for better speech boundaries");
    println!("   âœ… Context-aware processing implemented");

    sleep(Duration::from_millis(200)).await;

    // Pattern 2: Adaptive quality streaming
    println!("2ï¸âƒ£ Adaptive quality streaming...");
    let quality_levels = vec![
        "Low (8kHz)",
        "Medium (16kHz)",
        "High (44kHz)",
        "Ultra (96kHz)",
    ];
    for quality in &quality_levels {
        println!("   ğŸ“Š Switching to {} quality", quality);
        sleep(Duration::from_millis(100)).await;
    }
    println!("   âœ… Adaptive quality streaming completed");

    // Pattern 3: Multi-channel processing
    println!("3ï¸âƒ£ Multi-channel audio processing...");
    println!("   ğŸ”Š Processing stereo input (L/R channels)");
    println!("   ğŸ­ Extracting spatial audio features");
    println!("   ğŸ“ Directional audio analysis");
    println!("   âœ… Multi-channel processing completed");

    sleep(Duration::from_millis(200)).await;

    // Pattern 4: Real-time effects chain
    println!("4ï¸âƒ£ Real-time audio effects chain...");
    let effects = vec!["Noise Gate", "Compressor", "EQ", "Limiter"];
    for effect in &effects {
        println!("   ğŸ›ï¸ Applying {} effect", effect);
        sleep(Duration::from_millis(75)).await;
    }
    println!("   âœ… Effects chain processing completed");

    // Pattern 5: Continuous learning adaptation
    println!("5ï¸âƒ£ Continuous learning adaptation...");
    println!("   ğŸ§  Analyzing user speech patterns");
    println!("   ğŸ¯ Adapting VAD threshold based on environment");
    println!("   ğŸ“ˆ Improving noise reduction over time");
    println!("   âœ… Adaptive learning completed");

    println!("\nğŸ‰ All advanced streaming patterns demonstrated!");
    Ok(())
}

// Helper function for error simulation
#[cfg(all(
    feature = "streaming-audio",
    feature = "multimodal",
    feature = "format-conversion"
))]
async fn simulate_error_recovery() -> Result<(), MullamaError> {
    println!("\nğŸ›¡ï¸ Error Recovery Simulation");
    println!("============================");

    let error_scenarios = vec![
        "Audio device disconnected",
        "Buffer overflow detected",
        "Network interruption",
        "Model processing timeout",
    ];

    for scenario in &error_scenarios {
        println!("âŒ Simulating: {}", scenario);
        sleep(Duration::from_millis(50)).await;
        println!("ğŸ”„ Recovering...");
        sleep(Duration::from_millis(100)).await;
        println!("âœ… Recovery successful");
        println!();
    }

    Ok(())
}
